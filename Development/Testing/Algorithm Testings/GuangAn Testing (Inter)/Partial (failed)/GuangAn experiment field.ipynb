{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuangAn:\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialise class \"\"\"\n",
    "        self._initialise_objects()\n",
    "\n",
    "        # print('GuangAn Initialised')\n",
    "\n",
    "\n",
    "\n",
    "    def _initialise_objects(self):\n",
    "        \"\"\" Helper to initialise objects \"\"\"\n",
    "\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.val_x = None\n",
    "        self.val_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.checked_dict = None\n",
    "        self.model = None\n",
    "        self.parameter_ranges = None\n",
    "        self.hyperparameters = None\n",
    "        self.tuning_result_saving_address = None\n",
    "        self.object_saving_address = None\n",
    "        self._up_to = 0\n",
    "        self._seed = 19040822\n",
    "        self.best_score = -np.inf\n",
    "        self.best_combo = None\n",
    "        self.best_clf = None\n",
    "        self.clf_type = None\n",
    "        self._tune_features = False\n",
    "        self.non_tuneable_parameter_choices = list()\n",
    "        self.transform = None\n",
    "        self._is_new_best = 0\n",
    "        self.best_model_saving_address = None\n",
    "        self._feature_combo_n_index_map = None\n",
    "\n",
    "        self.regression_extra_output_columns = ['Train r2', 'Val r2', 'Test r2', \n",
    "            'Train RMSE', 'Val RMSE', 'Test RMSE', 'Train MAPE', 'Val MAPE', 'Test MAPE', 'Time']\n",
    "        self.classification_extra_output_columns = ['Train accu', 'Val accu', 'Test accu', \n",
    "            'Train balanced_accu', 'Val balanced_accu', 'Test balanced_accu', 'Train f1', 'Val f1', 'Test f1', \n",
    "            'Train precision', 'Val precision', 'Test precision', 'Train recall', 'Val recall', 'Test recall', 'Time']\n",
    "\n",
    "        \n",
    "\n",
    "    def read_in_data(self, train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "        \"\"\" Reads in train validate test data for tuning \"\"\"\n",
    "\n",
    "        self.train_x = train_x\n",
    "        print(\"Read in Train X data\")\n",
    "\n",
    "        self.train_y = train_y\n",
    "        print(\"Read in Train x data\")\n",
    "\n",
    "        self.val_x = val_x\n",
    "        print(\"Read in Val X data\")\n",
    "\n",
    "        self.val_y = val_y\n",
    "        print(\"Read in Val y data\")\n",
    "\n",
    "        self.test_x = test_x\n",
    "        print(\"Read in Test X data\")\n",
    "\n",
    "        self.test_y = test_y\n",
    "        print(\"Read in Test y data\")\n",
    "\n",
    "\n",
    "\n",
    "    def read_in_model(self, model, type):\n",
    "        \"\"\" Reads in underlying model object for tuning, and also read in what type of model it is \"\"\"\n",
    "\n",
    "        assert type == 'Classification' or type == 'Regression' # check\n",
    "\n",
    "        # record\n",
    "        self.model = model\n",
    "        self.clf_type = type \n",
    "\n",
    "        print(f'Successfully read in model {self.model}, which is a {self.clf_type} model')\n",
    "\n",
    "\n",
    "\n",
    "    def set_hyperparameters(self, parameter_ranges_orig):\n",
    "        \"\"\" Input hyperparameter choices \"\"\"\n",
    "\n",
    "        self.parameter_ranges = parameter_ranges_orig\n",
    "        self._sort_hyperparameter_ranges()\n",
    "        \n",
    "\n",
    "        self.hyperparameters = list(self.parameter_ranges.keys())\n",
    "\n",
    "        self.categorical = {hp:False for hp in self.hyperparameters}\n",
    "        self.transform = {hp:False for hp in self.hyperparameters}\n",
    "\n",
    "        self._get_checked_dict()\n",
    "        self._setup_tuning_result_df()\n",
    "\n",
    "        self.original_bounds = [(self.parameter_ranges[key], key) for key in self.parameter_ranges]\n",
    "\n",
    "        # print(\"Successfully recorded hyperparameter choices\")\n",
    "\n",
    "\n",
    "\n",
    "    def _sort_hyperparameter_ranges(self):\n",
    "        \"\"\" Helper to ensure all hyperparameter choice values are in order from lowest to highest \"\"\"\n",
    "\n",
    "        for key in self.parameter_ranges:\n",
    "            tmp = copy.deepcopy(list(self.parameter_ranges[key]))\n",
    "            tmp.sort()\n",
    "            if type(self.parameter_ranges[key]) is set:\n",
    "                self.parameter_ranges[key] = set(tmp)\n",
    "            else:\n",
    "                self.parameter_ranges[key] = tuple(tmp)\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_tuning_result_df(self):\n",
    "        \"\"\" Helper to set up tuning result dataframe \"\"\"\n",
    "\n",
    "        tune_result_columns = copy.deepcopy(self.hyperparameters)\n",
    "\n",
    "        if self._tune_features:\n",
    "            tune_result_columns.append('feature combo ningxiang score')\n",
    "\n",
    "        # Different set of metric columns for different types of models\n",
    "        if self.clf_type == 'Classification':\n",
    "            tune_result_columns.extend(self.classification_extra_output_columns)\n",
    "        elif self.clf_type == 'Regression':\n",
    "            tune_result_columns.extend(self.regression_extra_output_columns)\n",
    "\n",
    "        self.tuning_result = pd.DataFrame({col:list() for col in tune_result_columns})\n",
    "\n",
    "        \n",
    "    \n",
    "    def _get_checked_dict(self):\n",
    "        \"\"\" Helper to set up checked list \"\"\"\n",
    "\n",
    "        self.checked_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "    def set_non_tuneable_hyperparameters(self, non_tuneable_hyperparameter_choice):\n",
    "        \"\"\" Input Non tuneable hyperparameter choice \"\"\"\n",
    "\n",
    "        if type(non_tuneable_hyperparameter_choice) is not dict:\n",
    "            print('non_tuneable_hyeprparameters_choice must be dict, please try again')\n",
    "            return\n",
    "        \n",
    "        for nthp in non_tuneable_hyperparameter_choice:\n",
    "            if type(non_tuneable_hyperparameter_choice[nthp]) in (set, list, tuple, dict):\n",
    "                print('non_tuneable_hyperparameters_choice must not be of array-like type')\n",
    "                return\n",
    "\n",
    "        self.non_tuneable_parameter_choices = non_tuneable_hyperparameter_choice\n",
    "\n",
    "        print(\"Successfully recorded non_tuneable_hyperparameter choices\")\n",
    "\n",
    "\n",
    "\n",
    "    def read_in_transform(self, transform_update):\n",
    "        \"\"\" Function to read in transformation settings \"\"\"\n",
    "\n",
    "        if not self.hyperparameters:\n",
    "            print(\"Missing hyperparameter choices, please run .set_hyperparameters() first\")\n",
    "            return\n",
    "\n",
    "        if type(transform_update) is not dict:\n",
    "            print('transform_update should be a dict, please re-enter')\n",
    "            return\n",
    "\n",
    "        for key in transform_update:\n",
    "            self.transform[key] = transform_update[key]\n",
    "\n",
    "        print('Updated transform dictionary:', self.transform)\n",
    "\n",
    "    \n",
    "\n",
    "    def read_in_categorical(self, categorical_update):\n",
    "        \"\"\" Function to read in categorical settings \"\"\"\n",
    "\n",
    "        if not self.hyperparameters:\n",
    "            print(\"Missing hyperparameter choices, please run .set_hyperparameters() first\")\n",
    "            return\n",
    "\n",
    "        if type(categorical_update) is not list:\n",
    "            print('categorical_update should be a list, please re-enter')\n",
    "            return\n",
    "\n",
    "        for key in categorical_update:\n",
    "            self.categorical[key] = True\n",
    "            \n",
    "            self.parameter_ranges[key] = {'values': tuple(self.parameter_ranges[key])}\n",
    "        \n",
    "        self.original_bounds = [(self.parameter_ranges[key], key) for key in self.parameter_ranges]\n",
    "\n",
    "        print('Updated categorical dictionary:', self.categorical)\n",
    "        print('Updated original bounds dict:', self.original_bounds)\n",
    "\n",
    "    \n",
    "\n",
    "    def set_features(self, ningxiang_output):\n",
    "        \"\"\" Input features \"\"\"\n",
    "\n",
    "        if type(ningxiang_output) is not dict:\n",
    "            print(\"Please ensure NingXiang output is a dict\")\n",
    "            return\n",
    "        \n",
    "        if not self.hyperparameters:\n",
    "            print(\"Missing hyperparameter choices, please run .set_hyperparameters() first\")\n",
    "            return\n",
    "        \n",
    "        for feature in list(ningxiang_output.keys())[-1]:\n",
    "            if feature not in list(self.train_x.columns):\n",
    "                print(f'feature {feature} in ningxiang output is not in train_x. Please try again')\n",
    "                return\n",
    "            if feature not in list(self.val_x.columns):\n",
    "                print(f'feature {feature} in ningxiang output is not in val_x. Please try again')\n",
    "                return\n",
    "            if feature not in list(self.test_x.columns):\n",
    "                print(f'feature {feature} in ningxiang output is not in test_x. Please try again')\n",
    "                return\n",
    "\n",
    "        \n",
    "        # sort ningxiang just for safety, and store up\n",
    "        ningxiang_output_sorted = self._sort_features(ningxiang_output)\n",
    "        self.feature_n_ningxiang_score_dict = ningxiang_output_sorted\n",
    "\n",
    "        # activate this switch\n",
    "        self._tune_features = True\n",
    "\n",
    "        # update previous internal structures based on first set of hyperparameter choices\n",
    "        ##here used numbers instead of tuples as the values in parameter_choices; thus need another mapping to get map back to the features\n",
    "        self.parameter_ranges['features'] = set([i for i in range(len(ningxiang_output_sorted))])\n",
    "        self._feature_combo_n_index_map = {i: list(ningxiang_output_sorted.keys())[i] for i in range(len(ningxiang_output_sorted))}\n",
    "\n",
    "        self.hyperparameters = list(self.parameter_ranges.keys())\n",
    "        \n",
    "        self.categorical['features'] = False\n",
    "        self.transform['features'] = False\n",
    "\n",
    "        self._get_checked_dict()\n",
    "        self._setup_tuning_result_df()\n",
    "\n",
    "        self.original_bounds = [(self.parameter_ranges[key], key) for key in self.parameter_ranges]\n",
    "\n",
    "        print(\"Successfully recorded tuneable feature combination choices and updated relevant internal structures\")\n",
    "\n",
    "\n",
    "    \n",
    "    def _sort_features(self, ningxiang_output):\n",
    "        \"\"\" Helper for sorting features based on NingXiang values (input dict output dict) \"\"\"\n",
    "\n",
    "        ningxiang_output_list = [(key, ningxiang_output[key]) for key in ningxiang_output]\n",
    "\n",
    "        ningxiang_output_list.sort(key = lambda x:x[1])\n",
    "\n",
    "        ningxiang_output_sorted = {x[0]:x[1] for x in ningxiang_output_list}\n",
    "\n",
    "        return ningxiang_output_sorted\n",
    "\n",
    "\n",
    "\n",
    "    def get_coords_from_bounds(self, bounds):\n",
    "        \"\"\" Function to get initial coordinates to tune \"\"\"\n",
    "\n",
    "        # Setup the initial list that is used in classic JiaXing combo getting algorithm\n",
    "        if type(bounds[0][0]) is tuple:\n",
    "            boundary_coordinates = [[bounds[0][0][i]] for i in range(2)]\n",
    "        elif type(bounds[0][0]) is set: \n",
    "            if len(bounds[0][0]) == 1:\n",
    "                boundary_coordinates = [[list(bounds[0][0])[0]]]\n",
    "            else:\n",
    "                boundary_coordinates = [[min(list(bounds[0][0]))], [max(list(bounds[0][0]))]]\n",
    "        elif type(bounds[0][0]) is dict:\n",
    "            boundary_coordinates = [[bounds[0][0]['values'][i]] for i in range(len(bounds[0][0]['values']))]\n",
    "\n",
    "        # Second part of classic JiaXing combo getting algorithm\n",
    "        for i in range(1, len(bounds)):\n",
    "            old_boundary_coordinates = copy.deepcopy(boundary_coordinates)\n",
    "            boundary_coordinates = list()\n",
    "\n",
    "            values = bounds[i]\n",
    "\n",
    "            for init_coord in old_boundary_coordinates:\n",
    "                if type(values[0]) is tuple: # tuple: continuous values\n",
    "                    for value in values[0]:\n",
    "                        tmp = copy.deepcopy(init_coord)\n",
    "                        tmp.append(value)\n",
    "                        boundary_coordinates.append(tmp)\n",
    "\n",
    "                elif type(values[0]) is set: # set: semi-continuous values (ordinal or floats but not continuous) \n",
    "                    if len(values[0]) == 1:\n",
    "                        tmp = copy.deepcopy(init_coord)\n",
    "                        tmp.append(list(values[0])[0])\n",
    "                        boundary_coordinates.append(tmp)\n",
    "                    else:\n",
    "                        for value in [min(list(values[0])), max(list(values[0]))]:\n",
    "                            tmp = copy.deepcopy(init_coord)\n",
    "                            tmp.append(value)\n",
    "                            boundary_coordinates.append(tmp)\n",
    "                \n",
    "                elif type(values[0]) is dict: # dict: discrete values\n",
    "                    for value in values[0]['values']:\n",
    "                        tmp = copy.deepcopy(init_coord)\n",
    "                        tmp.append(value)\n",
    "                        boundary_coordinates.append(tmp)\n",
    "\n",
    "        return boundary_coordinates\n",
    "\n",
    "\n",
    "\n",
    "    def get_centre_components(self, bounds, categorical):\n",
    "        \"\"\" Helper that gets the centres of a bound as lists (considering for categorical) \"\"\"\n",
    "\n",
    "        # Classic JiaXing getting combo algorithm\n",
    "        centre_components = list()\n",
    "        tmp_cat = copy.deepcopy(categorical)\n",
    "        for i in range(len(bounds)):\n",
    "\n",
    "            if type(bounds[i][0]) is tuple or type(bounds[i][0]) is list: # tuple: continuous values\n",
    "                # take the mean value\n",
    "                centre_components.append(sum(bounds[i][0])/2)\n",
    "\n",
    "            elif type(bounds[i][0]) is set: # set: semi-continuous values (ordinal or floats but not continuous) \n",
    "                n_semicont_values = len(bounds[i][0])\n",
    "                if n_semicont_values <= 2:\n",
    "\n",
    "                    # just input the tuple(set) as the centre (which will be recognised as two discrete)\n",
    "                    centre_components.append(tuple(bounds[i][0]))\n",
    "\n",
    "                    # set categorical of this variable to true because no more semi-continuous values in between the current bounds\n",
    "                    tmp_cat[self.hyperparameters[i]] = True \n",
    "\n",
    "                else:\n",
    "                    # take the middle\n",
    "                    components_list = list(bounds[i][0])\n",
    "                    components_list.sort()\n",
    "                    centre_components.append(components_list[n_semicont_values//2])\n",
    "\n",
    "            elif type(bounds[i][0]) is dict: # dict: discrete values\n",
    "                # input the value (a set) which will be recognised as discrete\n",
    "                centre_components.append(bounds[i][0]['values'])\n",
    "        \n",
    "        # returns 1. components that can be unpacked into multiple centres; 2. new categorical labels\n",
    "        return centre_components, tmp_cat\n",
    "\n",
    "\n",
    "\n",
    "    def unpack_centre(self, centre_components):\n",
    "        \"\"\" Helper to unpack centre components into centre \"\"\"\n",
    "\n",
    "        # Classic JiaXing algorithm for getting all combinations\n",
    "        centres = [[]]\n",
    "        for i in range(len(centre_components)):\n",
    "            old_centres = copy.deepcopy(centres)\n",
    "            centres = list()\n",
    "            if type(centre_components[i]) is tuple:\n",
    "                for obj in centre_components[i]:\n",
    "                    for cent in old_centres:\n",
    "                        tmp_cent = copy.deepcopy(cent)\n",
    "                        tmp_cent.append(obj)\n",
    "                        centres.append(tmp_cent)\n",
    "            else:\n",
    "                for cent in old_centres:\n",
    "                    tmp_cent = copy.deepcopy(cent)\n",
    "                    tmp_cent.append(centre_components[i])\n",
    "                    centres.append(tmp_cent)\n",
    "\n",
    "        return [tuple(centre) for centre in centres]\n",
    "\n",
    "\n",
    "    \n",
    "    def get_categorical(self, new_cat, boundaries):\n",
    "        \"\"\" Helper to get all combos of categorical feature's values (for use in OLS) \"\"\"\n",
    "\n",
    "        # Classic JiaXing algorithm for getting all combinations\n",
    "        out = [[]]\n",
    "        for hyperparameter in new_cat:\n",
    "            if new_cat[hyperparameter] is True:\n",
    "                old_out = copy.deepcopy(out)\n",
    "                out = list()\n",
    "\n",
    "                val_list = list(boundaries[hyperparameter])\n",
    "                val_list_unique = list(set(val_list))\n",
    "                val_list_unique.sort()\n",
    "\n",
    "                for val in val_list_unique:\n",
    "                    for lst in old_out:\n",
    "                        tmp = copy.deepcopy(lst)\n",
    "                        tmp.append(val)\n",
    "                        out.append(tmp)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def get_new_bounds(self, bounds, centre, categorical):\n",
    "        \"\"\" Function to get new bounds \"\"\"\n",
    "\n",
    "        # get the range components that make the 2^d bounds\n",
    "        range_components = self.get_range_components(bounds, centre, categorical)\n",
    "\n",
    "        # make the new bounds from components\n",
    "        new_bounds = self.make_bounds(range_components)\n",
    "\n",
    "        return new_bounds\n",
    "\n",
    "\n",
    "\n",
    "    def get_range_components(self, bounds, centre, categorical):\n",
    "        \"\"\" Helper that gets the range components \"\"\"\n",
    "\n",
    "        range_components = list()\n",
    "        for i in range(len(bounds)):\n",
    "            \n",
    "            if type(bounds[i][0]) is tuple:\n",
    "                lower_range = (bounds[i][0][0], centre[i]) \n",
    "                upper_range = (centre[i], bounds[i][0][1]) \n",
    "\n",
    "                ranges = (lower_range, upper_range)\n",
    "\n",
    "                range_components.append((ranges, bounds[i][1]))\n",
    "            \n",
    "            elif type(bounds[i][0]) is set:\n",
    "                \n",
    "                if categorical[self.hyperparameters[i]] is False:\n",
    "                    lower_range_min_max = (min(list(bounds[i][0])), centre[i]) \n",
    "                    upper_range_min_max = (centre[i], max((bounds[i][0]))) \n",
    "                    lower_range = list()\n",
    "                    for orig_val in list(self.parameter_ranges[self.hyperparameters[i]]):\n",
    "                        if orig_val <= lower_range_min_max[1] and lower_range_min_max[0] <= orig_val:\n",
    "                            lower_range.append(orig_val)\n",
    "                    lower_range.sort()\n",
    "                    lower_range = set(lower_range)\n",
    "                    \n",
    "                    upper_range = list()\n",
    "                    for orig_val in list(self.parameter_ranges[self.hyperparameters[i]]):\n",
    "                        if orig_val <= upper_range_min_max[1] and upper_range_min_max[0] <= orig_val:\n",
    "                            upper_range.append(orig_val)\n",
    "                    upper_range.sort()\n",
    "                    upper_range = set(upper_range)\n",
    "\n",
    "                    ranges = (lower_range, upper_range)\n",
    "\n",
    "                    range_components.append((ranges, bounds[i][1]))\n",
    "                \n",
    "                else:\n",
    "                    ranges = bounds[i][0]\n",
    "                    range_components.append((ranges, bounds[i][1]))\n",
    "\n",
    "\n",
    "            elif type(bounds[i][0]) is dict:\n",
    "                \n",
    "                ranges = set(bounds[i][0]['values'])\n",
    "\n",
    "                range_components.append((ranges, bounds[i][1]))\n",
    "        \n",
    "        return range_components\n",
    "\n",
    "\n",
    "\n",
    "    def make_bounds(self, range_components, min_threshold = 0.1):\n",
    "        \"\"\" Helper that makes the bounds using range components \"\"\"\n",
    "\n",
    "        reach_threshold_tuple = 0 # set checks for reaching a minimum threshold\n",
    "        total_tuple = 0\n",
    "\n",
    "        # Algorithm to create all bounds\n",
    "        if type(range_components[0][0]) is tuple:\n",
    "          bounds = [[(range_components[0][0][i], range_components[0][1])] for i in range(2)] # hardcode cos bounds can only have 2 values\n",
    "          \n",
    "          if type(range_components[0][0][0]) is tuple:\n",
    "              total_tuple += 1\n",
    "              if range_components[0][0][0][1] - range_components[0][0][0][0] <= min_threshold:\n",
    "                  reach_threshold_tuple +=1\n",
    "\n",
    "        elif type(range_components[0][0]) is set:\n",
    "            tmp_tup = tuple(range_components[0][0])\n",
    "            bounds = [[({tmp_tup[i]}, range_components[0][1])] for i in range(len(tmp_tup))]\n",
    "\n",
    "        for i in range(1, len(range_components)):\n",
    "            old_bounds = copy.deepcopy(bounds)\n",
    "            bounds = list()\n",
    "\n",
    "            values = range_components[i]\n",
    "\n",
    "            for bound in old_bounds:\n",
    "                \n",
    "                if type(values[0]) == tuple:\n",
    "                    for value in values[0]:\n",
    "                        tmp = copy.deepcopy(bound)\n",
    "                        tmp.append((value, values[1]))\n",
    "\n",
    "                        bounds.append(tmp)\n",
    "\n",
    "                    if type(values[0][0]) is tuple:\n",
    "                        total_tuple += 1\n",
    "                        if values[0][0][1] - values[0][0][0] <= min_threshold:\n",
    "                            reach_threshold_tuple +=1\n",
    "                \n",
    "                elif type(values[0]) == set:\n",
    "                    for value in list(values[0]):\n",
    "                        tmp = copy.deepcopy(bound)\n",
    "                        tmp.append(({value}, values[1]))\n",
    "\n",
    "                        bounds.append(tmp)\n",
    "\n",
    "        if total_tuple and reach_threshold_tuple == total_tuple: # if total != 0 and all reached threshold\n",
    "            return False\n",
    "        \n",
    "        return bounds\n",
    "\n",
    "\n",
    "\n",
    "    def rebuild_bounds_to_original_format(self, tmp_boundary, new_cat):\n",
    "        \"\"\" Helper to rebuild current format of bounds (as a df) into original bound format\"\"\"\n",
    "\n",
    "        tmp_boundary = tmp_boundary.drop(['score'], axis = 1)\n",
    "        \n",
    "        bounds_original_format = list()\n",
    "        for col in tmp_boundary.columns:\n",
    "\n",
    "        # if already categorical: just keep it as categorical\n",
    "            if new_cat[col] == True: \n",
    "                bounds_original_format.append(({'values': tuple(set(tmp_boundary[col]))}, col))\n",
    "        \n",
    "            else:\n",
    "                col_vals = list(set(tmp_boundary[col]))\n",
    "            \n",
    "                if type(self.parameter_ranges[col]) is set:\n",
    "                    tmp = list()\n",
    "                    curr_val_max = max(col_vals)\n",
    "                    curr_val_min = min(col_vals)\n",
    "                    for orig_val in list(self.parameter_ranges[col]):\n",
    "                        if orig_val <= curr_val_max and curr_val_min <= orig_val:\n",
    "                            tmp.append(orig_val)\n",
    "                    tmp.sort()\n",
    "                    tmp = set(tmp)\n",
    "                    bounds_original_format.append((tmp, col))\n",
    "\n",
    "                else: # continuous values\n",
    "                    bounds_original_format.append(((min(col_vals), max(col_vals)), col))\n",
    "\n",
    "        return bounds_original_format\n",
    "\n",
    "\n",
    "\n",
    "    def _get_list_from_df(self, df):\n",
    "        \"\"\" Helper to get df rows into list form \"\"\"\n",
    "        \n",
    "        out = list()\n",
    "        for row in df.iterrows():\n",
    "            out.append(list(row[1].values))\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def _get_protective_bounds(self, bounds):\n",
    "        \"\"\" Helper to get protective bounds  - for boundaries\"\"\"\n",
    "\n",
    "        protective_bounds = list()\n",
    "        for bound in bounds:\n",
    "            if type(bound[0]) is dict: # categorical values become a set\n",
    "                protective_bounds.append(set(bound[0]['values']))\n",
    "            elif type(bound[0]) is set: # semi_categorical values become tuple\n",
    "                protective_bounds.append((min(list(bound[0])), max(list(bound[0]))))\n",
    "            elif type(bound[0]) is tuple: # continuous values stay as tuple\n",
    "                protective_bounds.append(bound[0])\n",
    "        \n",
    "        return protective_bounds\n",
    "\n",
    "\n",
    "\n",
    "    def _get_protective_bounds2(self, tmp_boundary, new_cat):\n",
    "        \"\"\" Helper to get protective bounds - for centres \"\"\"\n",
    "\n",
    "        protective_bounds = list()\n",
    "        for col in tmp_boundary.columns:\n",
    "            if col == 'score':\n",
    "                 continue\n",
    "            \n",
    "            col_values = list(tmp_boundary[col])\n",
    "\n",
    "            if new_cat[col]: # categorical values - only one left - set\n",
    "                protective_bounds.append({col_values[0],})\n",
    "            \n",
    "            else:  # continuous values - tuple\n",
    "                protective_bounds.append([min(col_values), max(col_values)])\n",
    "                 \n",
    "        \n",
    "        return protective_bounds\n",
    "    \n",
    "\n",
    "\n",
    "    def _in_protective_bounds(self, centre):\n",
    "        \"\"\" Determine whether centre is in protective_bounds \"\"\"\n",
    "\n",
    "        for i in range(len(centre)):\n",
    "            if type(self._protective_bounds[i]) is set: # categorical\n",
    "                if centre[i] not in self._protective_bounds[i]: # not matching any of the categorical values\n",
    "                    return False \n",
    "            else: # continuous values\n",
    "                if centre[i] > self._protective_bounds[i][1] or centre[i] < self._protective_bounds[i][0]: # outside of boundary tuple\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "    def _protective_bounds_to_original_bounds(self):\n",
    "        \"\"\" Helper to turn protective bounds back to original bounds \"\"\"\n",
    "\n",
    "        protective_to_original_bounds = list()\n",
    "        for i in range(len(self._protective_bounds)):\n",
    "            if type(self._protective_bounds[i]) is set: # discrete\n",
    "                protective_to_original_bounds.append(({'values': tuple(self._protective_bounds[i])}, self.original_bounds[i][1]))\n",
    "            else: # semicont or disc\n",
    "                if type(self.original_bounds[i][0]) is set:\n",
    "                    \n",
    "                    tmp = list()\n",
    "                    for val in self.original_bounds[i][0]:\n",
    "                        if val >= self._protective_bounds[i][0] and val <= self._protective_bounds[i][1]:\n",
    "                            tmp.append(val)\n",
    "\n",
    "                    protective_to_original_bounds.append((set(tmp), self.original_bounds[i][1]))\n",
    "                else:\n",
    "                    protective_to_original_bounds.append((tuple(self._protective_bounds[i]), self.original_bounds[i][1]))\n",
    "        \n",
    "        return protective_to_original_bounds\n",
    "\n",
    "\n",
    "\n",
    "    def tune(self, key_stats_only = False):\n",
    "        \"\"\" Begin tuning \"\"\"\n",
    "\n",
    "        # print(\"Begin Guidance\")\n",
    "\n",
    "        self.key_stats_only = key_stats_only\n",
    "\n",
    "        self._round = 0\n",
    "\n",
    "        # start by putting original bounds into a list; this list is the object that will control whether algorithm has terminated\n",
    "        bounds_list = [self.original_bounds]\n",
    "\n",
    "        while bounds_list: # gets reset every time, so algo will keep running if there are bounds to operate on\n",
    "            # print(\"Round:\", self._round)\n",
    "\n",
    "            old_bounds_list = copy.deepcopy(bounds_list)\n",
    "            tmp_bounds_list = list()\n",
    "\n",
    "            for k in range(len(old_bounds_list)): # now run algorithm on every bound\n",
    "\n",
    "                # get the coordinates that define the bounds\n",
    "                coords_to_tune = self.get_coords_from_bounds(old_bounds_list[k]) \n",
    "\n",
    "                # get all coordinates into a DataFrame - used for getting boundary\n",
    "                boundaries = pd.DataFrame()\n",
    "                for coord in coords_to_tune:\n",
    "                    \n",
    "                    # combination that goes straight into OLS\n",
    "                    combo_OLS_dict = {self.hyperparameters[i]:[coord[i]] for i in range(len(self.hyperparameters))}\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(coord) in self.checked_dict:\n",
    "                        self._check_already_trained_best_score(tuple(coord))\n",
    "                        combo_OLS_dict['score'] = self.checked_dict[tuple(coord)]['score']\n",
    "                        \n",
    "                    else:\n",
    "                        combo_dict = dict() # combination that gets transformed for searching\n",
    "                        for i in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[i]] == '10^':\n",
    "                                combo_dict[self.hyperparameters[i]] = [10**coord[i]]\n",
    "                    \n",
    "                            else:\n",
    "                                combo_dict[self.hyperparameters[i]] = [coord[i]] \n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(combo_dict)\n",
    "                        combo_OLS_dict['score'] = self.val_score\n",
    "\n",
    "                        if self._is_new_best:\n",
    "                            self._protective_bounds = self._get_protective_bounds(old_bounds_list[k])\n",
    "\n",
    "                        # store its metadata into checked_dict\n",
    "                        self.checked_dict[tuple(coord)] = {'score': self.val_score}\n",
    "\n",
    "                    # put this coord into df containing all boundaries (for later sliming depending on centre, and then OLS)\n",
    "                    tmp_boundary = pd.DataFrame(combo_OLS_dict)\n",
    "                    boundaries = boundaries.append(tmp_boundary)\n",
    "\n",
    "                if len(boundaries) == 1: # if only one coordinate in this boundary (i.e. all categorical)\n",
    "                    # print('ONE!\\n')\n",
    "                    continue\n",
    "\n",
    "                # get the components that make up the centre (as well as new categories); and then unpack them into centres\n",
    "                centre_components, new_cat = self.get_centre_components(old_bounds_list[k], self.categorical) # 加进去 - 改 for bound bounds with index\n",
    "                \n",
    "                centres = self.unpack_centre(centre_components)\n",
    "\n",
    "                # get the categorical features' values into a list for use in OLS preparation\n",
    "                categorical_value_list = self.get_categorical(new_cat, boundaries)\n",
    "\n",
    "                for i in range(len(centres)): # run through each different centre\n",
    "\n",
    "                    # create a dataframe version of centre (so we could put it into OLS)\n",
    "                    centre_OLS_df = pd.DataFrame({self.hyperparameters[j]:[centres[i][j]] for j in range(len(centres[i]))})\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(centres[i]) in self.checked_dict:\n",
    "                        self._check_already_trained_best_score(tuple(centres[i]))\n",
    "                        actual_centre_score = self.checked_dict[tuple(centres[i])]['score']\n",
    "                        \n",
    "                    else:\n",
    "                        centre_df = dict()\n",
    "                        for j in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[j]] == '10^':\n",
    "                                centre_df[self.hyperparameters[j]] = [10**centres[i][j]]\n",
    "                    \n",
    "                            else:\n",
    "                                centre_df[self.hyperparameters[j]] = [centres[i][j]]\n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(centre_df) \n",
    "                        actual_centre_score = self.val_score\n",
    "\n",
    "                        # store its metadata into checked_list\n",
    "                        self.checked_dict[tuple(centres[i])] = {'score': self.val_score}\n",
    "\n",
    "                    # copy the boundary dataframes - to turn into the correct training data for OLS (one lm model for each centre)\n",
    "                    tmp_boundary = copy.deepcopy(boundaries)\n",
    "                    tmp_boundary_drop = copy.deepcopy(boundaries)\n",
    "                    \n",
    "                    n_cat = 0\n",
    "                    for j in range(len(new_cat)):\n",
    "                \n",
    "                        if new_cat[self.hyperparameters[j]] == True:\n",
    "\n",
    "                            tmp_boundary = tmp_boundary[tmp_boundary[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop[tmp_boundary_drop[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop.drop([self.hyperparameters[j]], axis = 1)\n",
    "                            centre_OLS_df = centre_OLS_df.drop([self.hyperparameters[j]], axis=1)\n",
    "                            n_cat += 1\n",
    "\n",
    "                    tmp_boundary_X = tmp_boundary_drop.drop(['score'], axis = 1)\n",
    "                    tmp_boundary_y = tmp_boundary_drop['score']\n",
    "\n",
    "                    OLS = sm.OLS(tmp_boundary_y, tmp_boundary_X).fit()\n",
    "                    pred_centre_score = OLS.predict(centre_OLS_df)[0]\n",
    "                    # print('Pred centre score:', pred_centre_score)\n",
    "                    # print('Actual centre score:', actual_centre_score, '\\n')\n",
    "\n",
    "                    if self._is_new_best:\n",
    "                        self._protective_bounds = self._get_protective_bounds2(tmp_boundary, new_cat)\n",
    "                        \n",
    "                    if self._round >= 3:\n",
    "                        if actual_centre_score < 0:\n",
    "                            # print('ACTUAL NEG AFTER 3 ROUNDS!\\n')\n",
    "                            continue\n",
    "                    \n",
    "                    if actual_centre_score >= pred_centre_score-0.005 and actual_centre_score <= pred_centre_score+0.005:\n",
    "                        # print('FIT!\\n')\n",
    "                        pass\n",
    "\n",
    "                    else:\n",
    "                        bounds_original_format = self.rebuild_bounds_to_original_format(tmp_boundary, new_cat)\n",
    "                        new_bounds_list = self.get_new_bounds(bounds_original_format, centres[i], new_cat)\n",
    "\n",
    "                        if new_bounds_list != False:\n",
    "                            for new_bounds in new_bounds_list:\n",
    "                                tmp_bounds_list.append((new_bounds, actual_centre_score))\n",
    "                    \n",
    "            tmp_bounds_list.sort(key=lambda x:x[1], reverse=True)\n",
    "            n_accept = max(64, 2**len(self.hyperparameters))\n",
    "            tmp_bounds_list = tmp_bounds_list[:n_accept]\n",
    "            bounds_list = [x[0] for x in tmp_bounds_list]        \n",
    "            \n",
    "            self._round += 1\n",
    "\n",
    "\n",
    "        # Cruise algorithm\n",
    "        # print(\"Begin Cruise\")\n",
    "\n",
    "        cruise_bounds = [self._protective_bounds_to_original_bounds()]\n",
    "\n",
    "        run_through = True\n",
    "\n",
    "        while cruise_bounds: # gets reset every time, so algo will keep running if there are bounds to operate on\n",
    "            # print('cruise round')\n",
    "            if run_through == True:\n",
    "                old_max_bounds = cruise_bounds[0]\n",
    "\n",
    "            old_cruise_bounds = copy.deepcopy(cruise_bounds)\n",
    "            tmp_bounds_list = list()\n",
    "\n",
    "            for k in range(len(old_cruise_bounds)): # now run algorithm on every bound\n",
    "                # print('old_cruise_bound:', old_cruise_bounds[k])\n",
    "                # get the coordinates that define the bounds\n",
    "                coords_to_tune = self.get_coords_from_bounds(old_cruise_bounds[k]) \n",
    "\n",
    "                # get all coordinates into a DataFrame - used for getting boundary\n",
    "                boundaries = pd.DataFrame()\n",
    "                for coord in coords_to_tune:\n",
    "                    \n",
    "                    # combination that goes straight into OLS\n",
    "                    combo_OLS_dict = {self.hyperparameters[i]:[coord[i]] for i in range(len(self.hyperparameters))}\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(coord) in self.checked_dict:\n",
    "                        self._check_already_trained_best_score(coord)\n",
    "                        combo_OLS_dict['score'] = self.checked_dict[tuple(coord)]['score']\n",
    "                        \n",
    "                    else:\n",
    "                        combo_dict = dict() # combination that gets transformed for searching\n",
    "                        for i in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[i]] == '10^':\n",
    "                                combo_dict[self.hyperparameters[i]] = [10**coord[i]]\n",
    "                    \n",
    "                            else:\n",
    "                                combo_dict[self.hyperparameters[i]] = [coord[i]] \n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(combo_dict)\n",
    "                        combo_OLS_dict['score'] = self.val_score\n",
    "\n",
    "                        if self._is_new_best:\n",
    "                            self._protective_bounds = self._get_protective_bounds(old_cruise_bounds[k])\n",
    "\n",
    "                        # store its metadata into checked_dict\n",
    "                        self.checked_dict[tuple(coord)] = {'score': self.val_score}\n",
    "\n",
    "                    # put this coord into df containing all boundaries (for later sliming depending on centre, and then OLS)\n",
    "                    tmp_boundary = pd.DataFrame(combo_OLS_dict)\n",
    "                    boundaries = boundaries.append(tmp_boundary)\n",
    "                \n",
    "                if len(boundaries) == 1: # if only one coordinate in this boundary (i.e. all categorical)\n",
    "                    # print('ONE!\\n')\n",
    "                    continue\n",
    "\n",
    "                # get the components that make up the centre (as well as new categories); and then unpack them into centres\n",
    "                centre_components, new_cat = self.get_centre_components(old_cruise_bounds[k], self.categorical) # 加进去 - 改 for bound bounds with index\n",
    "                # print(centre_components)\n",
    "                \n",
    "                centres = self.unpack_centre(centre_components)\n",
    "                # print(centres)\n",
    "\n",
    "                # get the categorical features' values into a list for use in OLS preparation\n",
    "                categorical_value_list = self.get_categorical(new_cat, boundaries)\n",
    "\n",
    "                for i in range(len(centres)): # run through each different centre\n",
    "                    # print(centres[i])\n",
    "\n",
    "                    # create a dataframe version of centre (so we could put it into OLS)\n",
    "                    centre_OLS_df = pd.DataFrame({self.hyperparameters[j]:[centres[i][j]] for j in range(len(centres[i]))})\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(centres[i]) in self.checked_dict:\n",
    "                        self._check_already_trained_best_score(tuple(centres[i]))\n",
    "                        actual_centre_score = self.checked_dict[tuple(centres[i])]['score'] #TODO: remove\n",
    "                        \n",
    "                    else:\n",
    "                        centre_df = dict()\n",
    "                        for j in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[j]] == '10^':\n",
    "                                centre_df[self.hyperparameters[j]] = [10**centres[i][j]]\n",
    "                    \n",
    "                            else:\n",
    "                                centre_df[self.hyperparameters[j]] = [centres[i][j]]\n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(centre_df) \n",
    "                        actual_centre_score = self.val_score #TODO: remove\n",
    "\n",
    "                        # store its metadata into checked_list\n",
    "                        self.checked_dict[tuple(centres[i])] = {'score': self.val_score}\n",
    "\n",
    "                    # copy the boundary dataframes - to turn into the correct training data for OLS (one lm model for each centre)\n",
    "                    tmp_boundary = copy.deepcopy(boundaries)\n",
    "                    tmp_boundary_drop = copy.deepcopy(boundaries)\n",
    "                    \n",
    "                    n_cat = 0\n",
    "                    for j in range(len(new_cat)):\n",
    "                \n",
    "                        if new_cat[self.hyperparameters[j]] == True:\n",
    "\n",
    "                            tmp_boundary = tmp_boundary[tmp_boundary[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop[tmp_boundary_drop[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop.drop([self.hyperparameters[j]], axis = 1)\n",
    "                            centre_OLS_df = centre_OLS_df.drop([self.hyperparameters[j]], axis=1)\n",
    "                            n_cat += 1\n",
    "\n",
    "                    tmp_boundary_X = tmp_boundary_drop.drop(['score'], axis = 1)\n",
    "                    tmp_boundary_y = tmp_boundary_drop['score']\n",
    "\n",
    "                    OLS = sm.OLS(tmp_boundary_y, tmp_boundary_X).fit()\n",
    "                    pred_centre_score = OLS.predict(centre_OLS_df)[0]\n",
    "                    # print('Pred centre score:', pred_centre_score)\n",
    "                    # print('Actual centre score:', actual_centre_score, '\\n') # TODO: delete this whole OLS part\n",
    "\n",
    "                    if self._is_new_best:\n",
    "                        self._protective_bounds = self._get_protective_bounds2(tmp_boundary, new_cat)\n",
    "\n",
    "                    if run_through == True:\n",
    "                        # run straight through - one more round\n",
    "                        bounds_original_format = self.rebuild_bounds_to_original_format(tmp_boundary, new_cat)\n",
    "                        new_bounds_list = self.get_new_bounds(bounds_original_format, centres[i], new_cat)\n",
    "\n",
    "                        if new_bounds_list != False:\n",
    "                            for new_bounds in new_bounds_list:\n",
    "                                tmp_bounds_list.append((new_bounds, actual_centre_score))\n",
    "\n",
    "\n",
    "\n",
    "            if run_through == False:\n",
    "                max_bounds = self._protective_bounds_to_original_bounds()\n",
    "                if max_bounds != old_max_bounds:\n",
    "                    cruise_bounds = [max_bounds]\n",
    "            else:\n",
    "                tmp_bounds_list.sort(key=lambda x:x[1], reverse=True)\n",
    "                n_accept = max(64, 2**len(self.hyperparameters))\n",
    "                tmp_bounds_list = tmp_bounds_list[:n_accept]\n",
    "                cruise_bounds = [x[0] for x in tmp_bounds_list] \n",
    "\n",
    "            run_through = not run_through\n",
    "\n",
    "\n",
    "        # # Display final information\n",
    "        # print(\"TUNING FINISHED\\n\")\n",
    "\n",
    "        # print('Max Score: \\n', self.best_score)\n",
    "        # print('Max Combo: \\n', self.best_combo)\n",
    "\n",
    "        # print('% Combos Checked:', int(len(self.checked_dict)))\n",
    "\n",
    "\n",
    "\n",
    "    # def _train_and_test_combo(self, combo):\n",
    "    #     \"\"\" Helper to train and test each combination as part of tune() \"\"\"\n",
    "\n",
    "        \n",
    "    #     params = {self.hyperparameters[i]:combo[self.hyperparameters[i]][0] for i in range(len(self.hyperparameters))}\n",
    "        \n",
    "    #     if self._tune_features == True:\n",
    "    #         del params['features']\n",
    "    #         tmp_train_x = self.train_x[list(self._feature_combo_n_index_map[combo['features'][0]])] \n",
    "    #         tmp_val_x = self.val_x[list(self._feature_combo_n_index_map[combo['features'][0]])]\n",
    "    #         tmp_test_x = self.test_x[list(self._feature_combo_n_index_map[combo['features'][0]])]\n",
    "\n",
    "    #         # add non tuneable parameters\n",
    "    #         for nthp in self.non_tuneable_parameter_choices:\n",
    "    #             params[nthp] = self.non_tuneable_parameter_choices[nthp]\n",
    "\n",
    "    #         # initialise object\n",
    "    #         clf = self.model(**params)\n",
    "\n",
    "    #         params['features'] = [list(self._feature_combo_n_index_map[combo['features'][0]])] \n",
    "    #         params['feature combo ningxiang score'] = self.feature_n_ningxiang_score_dict[self._feature_combo_n_index_map[combo['features'][0]]]\n",
    "\n",
    "    #     else:\n",
    "    #         tmp_train_x = self.train_x\n",
    "    #         tmp_val_x = self.val_x\n",
    "    #         tmp_test_x = self.test_x\n",
    "\n",
    "    #         # add non tuneable parameters\n",
    "    #         for nthp in self.non_tuneable_parameter_choices:\n",
    "    #             params[nthp] = self.non_tuneable_parameter_choices[nthp]\n",
    "\n",
    "    #         # initialise object\n",
    "    #         clf = self.model(**params)\n",
    "\n",
    "    #     # get time and fit\n",
    "    #     start = time.time()\n",
    "    #     clf.fit(tmp_train_x, self.train_y)\n",
    "    #     end = time.time()\n",
    "\n",
    "    #     # get predicted labels/values for three datasets\n",
    "    #     train_pred = clf.predict(tmp_train_x)\n",
    "    #     val_pred = clf.predict(tmp_val_x)\n",
    "    #     test_pred = clf.predict(tmp_test_x)\n",
    "\n",
    "    #     # get scores and time used\n",
    "    #     time_used = end-start\n",
    "\n",
    "    #     # build output dictionary and save result\n",
    "    #     df_building_dict = params\n",
    "\n",
    "\n",
    "    #     if self.clf_type == 'Regression':\n",
    "\n",
    "    #         train_score = val_score = test_score = train_rmse = val_rmse = test_rmse = train_mape = val_mape = test_mape = 0\n",
    "\n",
    "    #         try:\n",
    "    #             train_score = r2_score(self.train_y, train_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             val_score = r2_score(self.val_y, val_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             test_score = r2_score(self.test_y, test_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "            \n",
    "    #         try:\n",
    "    #             train_rmse = np.sqrt(mean_squared_error(self.train_y, train_pred))\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             val_rmse = np.sqrt(mean_squared_error(self.val_y, val_pred))\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             test_rmse = np.sqrt(mean_squared_error(self.test_y, test_pred))\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "    #         if self.key_stats_only == False:\n",
    "    #             try:\n",
    "    #                 train_mape = mean_absolute_percentage_error(self.train_y, train_pred)\n",
    "    #             except:\n",
    "    #                 pass\n",
    "    #             try:\n",
    "    #                 val_mape = mean_absolute_percentage_error(self.val_y, val_pred)\n",
    "    #             except:\n",
    "    #                 pass\n",
    "    #             try:\n",
    "    #                 test_mape = mean_absolute_percentage_error(self.test_y, test_pred)\n",
    "    #             except:\n",
    "    #                 pass\n",
    "            \n",
    "    #         df_building_dict['Train r2'] = [np.round(train_score, 6)]\n",
    "    #         df_building_dict['Val r2'] = [np.round(val_score, 6)]\n",
    "    #         df_building_dict['Test r2'] = [np.round(test_score, 6)]\n",
    "    #         df_building_dict['Train RMSE'] = [np.round(train_rmse, 6)]\n",
    "    #         df_building_dict['Val RMSE'] = [np.round(val_rmse, 6)]\n",
    "    #         df_building_dict['Test RMSE'] = [np.round(test_rmse, 6)]\n",
    "            \n",
    "    #         if self.key_stats_only == False:\n",
    "    #             df_building_dict['Train MAPE'] = [np.round(train_mape, 6)]\n",
    "    #             df_building_dict['Val MAPE'] = [np.round(val_mape, 6)]\n",
    "    #             df_building_dict['Test MAPE'] = [np.round(test_mape, 6)]\n",
    "\n",
    "        \n",
    "    #     elif self.clf_type == 'Classification':\n",
    "\n",
    "    #         train_score = val_score = test_score = train_bal_accu = val_bal_accu = test_bal_accu = train_f1 = val_f1 = test_f1 = \\\n",
    "    #             train_precision = val_precision = test_precision = train_recall = val_recall = test_recall = 0\n",
    "\n",
    "    #         try:    \n",
    "    #             train_score = accuracy_score(self.train_y, train_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             val_score = accuracy_score(self.val_y, val_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             test_score = accuracy_score(self.test_y, test_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "    #         try:\n",
    "    #             train_bal_accu = balanced_accuracy_score(self.train_y, train_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             val_bal_accu = balanced_accuracy_score(self.val_y, val_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             test_bal_accu = balanced_accuracy_score(self.test_y, test_pred)\n",
    "    #         except:\n",
    "    #             pass\n",
    "            \n",
    "    #         try:\n",
    "    #             train_f1 = f1_score(self.train_y, train_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             val_f1 = f1_score(self.val_y, val_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             test_f1 = f1_score(self.test_y, test_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "            \n",
    "    #         try:\n",
    "    #             train_precision = precision_score(self.train_y, train_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             val_precision = precision_score(self.val_y, val_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             test_precision = precision_score(self.test_y, test_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "    #         try:\n",
    "    #             train_recall = recall_score(self.train_y, train_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             val_recall = recall_score(self.val_y, val_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #         try:\n",
    "    #             test_recall = recall_score(self.test_y, test_pred, average='weighted')\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "    #         df_building_dict['Train accu'] = [np.round(train_score, 6)]\n",
    "    #         df_building_dict['Val accu'] = [np.round(val_score, 6)]\n",
    "    #         df_building_dict['Test accu'] = [np.round(test_score, 6)]\n",
    "    #         df_building_dict['Train balanced_accuracy'] = [np.round(train_bal_accu, 6)]\n",
    "    #         df_building_dict['Val balanced_accuracy'] = [np.round(val_bal_accu, 6)]\n",
    "    #         df_building_dict['Test balanced_accuracy'] = [np.round(test_bal_accu, 6)]\n",
    "    #         df_building_dict['Train f1'] = [np.round(train_f1, 6)]\n",
    "    #         df_building_dict['Val f1'] = [np.round(val_f1, 6)]\n",
    "    #         df_building_dict['Test f1'] = [np.round(test_f1, 6)]\n",
    "    #         df_building_dict['Train precision'] = [np.round(train_precision, 6)]\n",
    "    #         df_building_dict['Val precision'] = [np.round(val_precision, 6)]\n",
    "    #         df_building_dict['Test precision'] = [np.round(test_precision, 6)]\n",
    "    #         df_building_dict['Train recall'] = [np.round(train_recall, 6)]\n",
    "    #         df_building_dict['Val recall'] = [np.round(val_recall, 6)]\n",
    "    #         df_building_dict['Test recall'] = [np.round(test_recall, 6)]\n",
    "\n",
    "\n",
    "    #     df_building_dict['Time'] = [np.round(time_used, 2)]\n",
    "\n",
    "\n",
    "    #     tmp = pd.DataFrame(df_building_dict)\n",
    "\n",
    "    #     self.tuning_result = self.tuning_result.append(tmp)\n",
    "    #     self._save_tuning_result()\n",
    "\n",
    "    #     self._is_new_best = 0\n",
    "\n",
    "    #     # update best score stats\n",
    "    #     if val_score > self.best_score: \n",
    "    #         self.best_score = val_score\n",
    "    #         self.best_clf = clf\n",
    "    #         self.best_combo = combo\n",
    "    #         self._save_best_model()\n",
    "\n",
    "    #         self._is_new_best = 1\n",
    "\n",
    "    #     # add a new self variable compared to previous JiaXing classes\n",
    "    #     self.val_score = val_score\n",
    "\n",
    "    #     print(f'''Trained and Tested combination {self._up_to}: {combo}, taking {np.round(time_used, 2)} seconds to get val score of {np.round(val_score, 4)}\n",
    "    #     Current best combo: {self.best_combo} with val score {np.round(self.best_score, 4)}''')\n",
    "\n",
    "\n",
    "\n",
    "    def _train_and_test_combo(self, combo):\n",
    "        \"\"\" Helper to train and test each combination as part of tune() \"\"\"\n",
    "\n",
    "        params = {f'{i}':combo[self.hyperparameters[i]][0] for i in range(len(self.hyperparameters))}\n",
    "\n",
    "        # get predicted labels/values for three datasets\n",
    "        val_score = gpr.predict(pd.DataFrame(params))[0]\n",
    "\n",
    "        self._is_new_best = 0\n",
    "\n",
    "        # update best score stats\n",
    "        if val_score > self.best_score: \n",
    "            self.best_score = val_score\n",
    "            self.best_combo = combo\n",
    "\n",
    "            self._is_new_best = 1\n",
    "\n",
    "        # add a new self variable compared to previous JiaXing classes\n",
    "        self.val_score = val_score\n",
    "\n",
    "\n",
    "\n",
    "    def _save_tuning_result(self):\n",
    "        \"\"\" Helper to export tuning result csv \"\"\"\n",
    "\n",
    "        tuning_result_saving_address_strip = self.tuning_result_saving_address.split('.csv')[0]\n",
    "\n",
    "        self.tuning_result.to_csv(f'{tuning_result_saving_address_strip}.csv', index=False)\n",
    "\n",
    "\n",
    "    \n",
    "    def view_best_combo_and_score(self):\n",
    "        \"\"\" View best combination and its validation score \"\"\"\n",
    "        \n",
    "        print(f'(Current) Best combo: {self.best_combo} with val score {np.round(self.best_score, 4)}')\n",
    "\n",
    "    \n",
    "\n",
    "    def read_in_tuning_result_df(self, address): \n",
    "        \"\"\" Read in tuning result csv and read data into checked and result arrays \"\"\"\n",
    "\n",
    "        if self.parameter_ranges is None:\n",
    "            print(\"Missing parameter_ranges to build parameter_value_map_index, please run set_hyperparameters() first\")\n",
    "            return\n",
    "\n",
    "        if self.clf_type is None:\n",
    "            print('Missing clf_type. Please run .read_in_model() first.')\n",
    "            return\n",
    "\n",
    "        if self.transform is None:\n",
    "            print('Missing transform dict, Please run .read_in_transform() first.')\n",
    "            return\n",
    "\n",
    "        self.tuning_result = pd.read_csv(address)\n",
    "        print(f\"Successfully read in tuning result of {len(self.tuning_result)} rows\")\n",
    "        \n",
    "        self._up_to = 0\n",
    "\n",
    "        # read DataFrame data into internal governing DataFrames of GuangAn\n",
    "        for row in self.tuning_result.iterrows():\n",
    "            \n",
    "            self._up_to += 1\n",
    "\n",
    "            combo = list()\n",
    "            for hyperparam in self.hyperparameters:\n",
    "                if hyperparam == 'features':\n",
    "\n",
    "                    # reverse two dicts\n",
    "                    index_n_feature_combo_map = {self._feature_combo_n_index_map[key]:key for key in self._feature_combo_n_index_map}\n",
    "                    # special input\n",
    "                    combo.append(index_n_feature_combo_map[tuple(self._str_to_list(row[1]['features']))])\n",
    "                  \n",
    "                else:\n",
    "                    if self.transform[hyperparam] == '10^':\n",
    "                        combo.append(np.log10(row[1][hyperparam]))\n",
    "                    else:\n",
    "                        combo.append(row[1][hyperparam])\n",
    "\n",
    "            combo = tuple(combo)\n",
    "            \n",
    "            if self.clf_type == 'Regression': \n",
    "                self.checked_dict[combo] = {'score': row[1]['Val r2']}\n",
    "            elif self.clf_type == 'Classification':\n",
    "                self.checked_dict[combo] = {'score': row[1]['Val r2']}\n",
    "    \n",
    "    \n",
    "\n",
    "    def _check_already_trained_best_score(self, combo):\n",
    "        \"\"\" Helper for checking whether an already trained combo is best score \"\"\"\n",
    "        \n",
    "        combo = tuple(combo)\n",
    "\n",
    "        self.is_new_best = 0\n",
    "\n",
    "        # update best score stats\n",
    "        if self.checked_dict[combo]['score'] > self.best_score: \n",
    "            self.best_score = self.checked_dict[combo]['score']\n",
    "            self.best_clf = None\n",
    "            print(f\"As new Best Combo {combo} was read in, best_clf is set to None\")\n",
    "            self.best_combo = combo\n",
    "\n",
    "            self._is_new_best = 1\n",
    "\n",
    "        print(f'''Already Trained and Tested combination { {self.hyperparameters[i]:combo[i] for i in range(len(combo))} }, which had val score of {np.round(self.checked_dict[combo]['score'], 4)}\n",
    "        Current best combo: {self.best_combo} with val score {np.round(self.best_score, 4)}. \n",
    "        Has trained {self._up_to} combinations so far''')\n",
    "\n",
    "    \n",
    "\n",
    "    def _str_to_list(self, string):\n",
    "        \"\"\" Helper to convert string to list\"\"\"\n",
    "\n",
    "        out = list()\n",
    "        for feature in string.split(', '):\n",
    "            out.append(feature.strip('[').strip(']').strip(\"'\"))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "    def set_tuning_result_saving_address(self, address):\n",
    "        \"\"\" Read in where to save tuning object \"\"\"\n",
    "\n",
    "        self.tuning_result_saving_address = address\n",
    "        print('Successfully set tuning output address')\n",
    "\n",
    "\n",
    "    \n",
    "    def set_best_model_saving_address(self, address):\n",
    "        \"\"\" Read in where to save best model  \"\"\"\n",
    "\n",
    "        self.best_model_saving_address = address\n",
    "        print('Successfully set best model output address')\n",
    "\n",
    "    \n",
    "\n",
    "    def _save_best_model(self):\n",
    "        \"\"\" Helper to save best model as a pickle \"\"\"\n",
    "\n",
    "        best_model_saving_address_split = self.best_model_saving_address.split('.pickle')[0]\n",
    "\n",
    "        with open(f'{best_model_saving_address_split}.pickle', 'wb') as f:\n",
    "            pickle.dump(self.best_clf, f)\n",
    "\n",
    "\n",
    "    \n",
    "    def _set_object_saving_address(self, address):\n",
    "        \"\"\" Read in where to save the GuangAn object \"\"\"\n",
    "\n",
    "        self.object_saving_address = address\n",
    "        print('Successfully set object output address')\n",
    "\n",
    "\n",
    "\n",
    "    def export_guangan(self, address):\n",
    "        \"\"\" Export GuangAn object \"\"\"\n",
    "\n",
    "        self._set_object_saving_address(address)\n",
    "\n",
    "        # copy object and set big objects to None\n",
    "        object_save = copy.deepcopy(self)\n",
    "        \n",
    "        object_save.train_x = None\n",
    "        object_save.train_y = None\n",
    "        object_save.val_x = None\n",
    "        object_save.val_y = None\n",
    "        object_save.test_x = None\n",
    "        object_save.test_y = None\n",
    "        object_save._up_to = 0\n",
    "\n",
    "        # Export\n",
    "        object_saving_address_strip = self.object_saving_address.split('.pickle')[0]\n",
    "        with open(f'{object_saving_address_strip}.pickle', 'wb') as f:\n",
    "            pickle.dump(object_save, f)\n",
    "\n",
    "        print(f'Successfully exported GuangAn object as {self.object_saving_address}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING GROUND"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GuangAn Initialised\n"
     ]
    }
   ],
   "source": [
    "for batch in (1, 2,):\n",
    "\n",
    "    experiment_data = pd.DataFrame()\n",
    "    # experiment_data = pd.read_csv(f'./{yangzhou_version}-batch{batch}.csv')\n",
    "\n",
    "    targets = os.listdir(f'../../Testing/Synthetic Data (Interact)/Batch {batch}')\n",
    "    targets = [(int(file.split('-')[0]), file) for file in targets if file != '.DS_Store' and file[-5:] == '.json']\n",
    "    targets = sorted(targets, key = lambda x:x[0])\n",
    "    targets = [file[1] for file in targets]\n",
    "\n",
    "    for target in targets:\n",
    "        \n",
    "        target_parts = target.split('-')\n",
    "\n",
    "        if len(target_parts) == 7:\n",
    "            id = target_parts[0]\n",
    "            dim, d_val = target_parts[1].split('_')[1:3]\n",
    "            mean_type = target_parts[2].strip('M_')\n",
    "            sd_type = target_parts[3].strip('SD_')\n",
    "            interact_type = -float(target_parts[5])\n",
    "        else:\n",
    "            id = target_parts[0]\n",
    "            dim, d_val = target_parts[1].split('_')[1:3]\n",
    "            mean_type = target_parts[2].strip('M_')\n",
    "            sd_type = target_parts[3].strip('SD_')\n",
    "            interact_type = target_parts[4].strip('I_')\n",
    "        \n",
    "        with open(f'../../Testing/Synthetic Data (Interact)/Batch {batch}/{target}', 'r') as f:\n",
    "            json_input = json.load(f)\n",
    "\n",
    "        input_csv = pd.read_csv(f'../../Testing/Synthetic Data (Interact)/Batch {batch}/{target.split(\".csv\")[0]}.csv')\n",
    "\n",
    "        gpr = GaussianProcessRegressor().fit(input_csv.drop(['y'], axis = 1), input_csv['y'])\n",
    "        \n",
    "        guangan = GuangAn()\n",
    "\n",
    "        n_arg_val = json_input['num_arg_vals']\n",
    "        parameter_ranges = {f'{i}': (0, n_arg_val[i]-1) for i in range(len(n_arg_val))}\n",
    "\n",
    "        guangan.set_hyperparameters(parameter_ranges)\n",
    "\n",
    "        start = time.time()\n",
    "        guangan.tune()\n",
    "        end = time.time()\n",
    "        time_taken = end - start\n",
    "\n",
    "        best_found = 0\n",
    "        if guangan.best_score == json_input['max']['synth_max']:\n",
    "            best_found = 3\n",
    "        elif (json_input['max']['synth_max']-guangan.best_score) <= 0.5*float(sd_type):\n",
    "            best_found = 2\n",
    "        elif (json_input['max']['synth_max']-guangan.best_score) <= 0.005:\n",
    "            best_found = 1\n",
    "\n",
    "        \n",
    "        result = pd.DataFrame({'id':[id], 'num_arg_val':[n_arg_val], 'dim': [dim], 'd_val': [d_val], \n",
    "            'mean_type':[mean_type], 'sd_types':[sd_type], 'interact_type':[interact_type], '#searched':[int(len(guangan.checked_dict))], \n",
    "            'best_found':[best_found], \n",
    "            'diff':[json_input['max']['synth_max']-guangan.best_score], \n",
    "            \"time\": [time_taken]})\n",
    "\n",
    "        experiment_data = experiment_data.append(result)\n",
    "\n",
    "        print(target, 'testing finished')\n",
    "\n",
    "        experiment_data.to_csv(f'./drive/MyDrive/Testing/Experiment Results/Guangan-batch{batch}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
