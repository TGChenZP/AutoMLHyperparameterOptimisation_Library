{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [random.gauss(0, 1) for _ in range(10000)] \n",
    "x2 = [random.gauss(0, 1) for _ in range(10000)]\n",
    "y = [0.25*x1[i]+0.25*x2[i]+random.gauss(0, 1)  for i in range(len(x1))]\n",
    "\n",
    "data = pd.DataFrame({'x1':x1, 'x2':x2, 'y': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1    0.002836\n",
       "x2   -0.000030\n",
       "y    -0.002423\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1    1.010403\n",
       "x2    1.000040\n",
       "y     1.065001\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017221</td>\n",
       "      <td>0.234664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>0.017221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.244501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.234664</td>\n",
       "      <td>0.244501</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2         y\n",
       "x1  1.000000  0.017221  0.234664\n",
       "x2  0.017221  1.000000  0.244501\n",
       "y   0.234664  0.244501  1.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.x.iloc[idx].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "            n_hidden_layers, \n",
    "            hidden_layer_n_neurons, \n",
    "            activation_function, \n",
    "            dropout_prob, \n",
    "            input_size, \n",
    "            output_size, \n",
    "            batch_normalisation,\n",
    "            last_activation_function = 'relu',):\n",
    "\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.batch_normalisation = batch_normalisation\n",
    "\n",
    "        self.ACTIVATION_FUNCTIONS_MAP = {'relu': nn.ReLU(), \n",
    "                                        'sigmoid': nn.Sigmoid(), \n",
    "                                        'tanh': nn.Tanh(), \n",
    "                                        'softmax': nn.Softmax(dim=1)}\n",
    "\n",
    "        # Define layers\n",
    "        self.nn_layers = nn.ModuleList([nn.Linear(input_size, hidden_layer_n_neurons[0])])\n",
    "        if self.batch_normalisation:  \n",
    "            self.batch_norm_layers = nn.ModuleList([])\n",
    "        self.activation_layer = nn.ModuleList([self.ACTIVATION_FUNCTIONS_MAP[activation_function]])\n",
    "        self.dropout_layer = nn.ModuleList([nn.Dropout(dropout_prob)])\n",
    "        \n",
    "        for i in range(n_hidden_layers-1):\n",
    "            self.nn_layers.append(nn.Linear(hidden_layer_n_neurons[i], hidden_layer_n_neurons[i+1]))\n",
    "            self.batch_norm_layers.append(nn.BatchNorm1d(hidden_layer_n_neurons[i + 1]))\n",
    "            self.activation_layer.append(self.ACTIVATION_FUNCTIONS_MAP[activation_function])\n",
    "            self.dropout_layer.append(nn.Dropout(dropout_prob))\n",
    "\n",
    "        self.nn_layers.append(nn.Linear(hidden_layer_n_neurons[-1], output_size))\n",
    "        self.activation_layer.append(self.ACTIVATION_FUNCTIONS_MAP[last_activation_function])\n",
    "\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        out = self.nn_layers[0](x)\n",
    "\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            if i != 0:\n",
    "                out = self.nn_layers[i](out)\n",
    "            \n",
    "            if self.batch_normalisation:\n",
    "                if i != 0:  # Exclude batch normalization for the input layer\n",
    "                    out = self.batch_norm_layers[i - 1](out)\n",
    "\n",
    "            out = self.activation_layer[i](out)\n",
    "\n",
    "            if training:\n",
    "                out = self.dropout_layer[i](out)\n",
    "            \n",
    "        out = self.nn_layers[-1](out)\n",
    "        out = self.activation_layer[-1](out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_const_pt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_hidden_layers, \n",
    "                 activation, \n",
    "                 lambda_lasso, \n",
    "                 epoch_batch_size, \n",
    "                 batch_size,\n",
    "                 learning_rate, \n",
    "                 num_epochs, \n",
    "                 random_state, \n",
    "                 dropout_prob,\n",
    "                 hidden_layer_n_neuron,\n",
    "                 batch_normalisation = False,\n",
    "                 verbose = False,\n",
    "                 gpu = False,\n",
    "                 gpu_id = 0,\n",
    "                 loss_function='MSE',\n",
    "                 last_activation_function = 'relu',\n",
    "                 **kwargs):\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.activation = activation\n",
    "        self.lambda_lasso = lambda_lasso\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.epoch_batch_size = epoch_batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.hidden_layer_n_neuron = hidden_layer_n_neuron\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.verbose = verbose\n",
    "        self.loss_function = loss_function\n",
    "        self.last_activation_function = last_activation_function\n",
    "        self.batch_normalisation = batch_normalisation\n",
    "        \n",
    "        self.gpu = gpu\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "\n",
    "        self.LOSS_FUNCTIONS_MAP = {'MSE': nn.MSELoss(), \n",
    "                                    'MAE': nn.L1Loss(), \n",
    "                                    'Huber': nn.SmoothL1Loss()}\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, train_x, train_y, initial_model = None):\n",
    "\n",
    "\n",
    "        self.hidden_layer_sizes = [self.hidden_layer_n_neuron for i in range(self.n_hidden_layers)]\n",
    "\n",
    "        if type(train_y) == pd.core.frame.DataFrame:\n",
    "            self.output_size = len(train_y.columns)\n",
    "        else:\n",
    "            self.output_size = 1\n",
    "        \n",
    "        self.input_size = len(train_x.columns)\n",
    "\n",
    "       \n",
    "        # Create the model\n",
    "        self.model = DenseNeuralNetwork(self.n_hidden_layers, \n",
    "                                        self.hidden_layer_sizes, \n",
    "                                        self.activation, \n",
    "                                        self.dropout_prob, \n",
    "                                        self.input_size, \n",
    "                                        self.output_size,\n",
    "                                        self.last_activation_function,\n",
    "                                        self.batch_normalisation)\n",
    "\n",
    "        if initial_model is not None:\n",
    "            self.model.load_state_dict(initial_model.model.state_dict())        \n",
    "        \n",
    "        if self.gpu:\n",
    "            self.model.cuda(self.gpu_id)\n",
    "\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = self.LOSS_FUNCTIONS_MAP['MSE']\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Create the custom datasets\n",
    "        train_dataset = CustomDataset(train_x, train_y)\n",
    "\n",
    "        self.seeds = np.random.randint(10000000, size = self.num_epochs)\n",
    "\n",
    "        \n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.num_epochs):\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, worker_init_fn=lambda _: torch.manual_seed(self.seeds[epoch]))\n",
    "\n",
    "            instances_learnt_so_far = 0\n",
    "\n",
    "            for batch_idx, (batch_train_x, batch_train_y) in enumerate(train_loader):\n",
    "                \n",
    "                if self.gpu:\n",
    "                    batch_train_x, batch_train_y = batch_train_x.cuda(self.gpu_id), batch_train_y.cuda(self.gpu_id)\n",
    "                # Forward pass\n",
    "                outputs = self.model(batch_train_x)\n",
    "                target = batch_train_y.view(-1, 1)  # Reshape target tensor to match the size of the output\n",
    "                loss = criterion(outputs, target)\n",
    "\n",
    "                # Lasso regularization\n",
    "                l1_regularization = torch.tensor(0.)\n",
    "                for param in self.model.parameters():\n",
    "                    l1_regularization += torch.norm(param, p=1)\n",
    "                loss += self.lambda_lasso * l1_regularization\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                instances_learnt_so_far += self.batch_size\n",
    "                \n",
    "                if instances_learnt_so_far >= self.epoch_batch_size:\n",
    "                    break\n",
    "        \n",
    "            # Print the progress\n",
    "            if self.verbose:\n",
    "                if (epoch + 1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        x_tensor = torch.Tensor(x.values)\n",
    "\n",
    "        if self.gpu:\n",
    "            x_tensor = x_tensor.cuda(self.gpu_id)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(x_tensor, training=False)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DNN_shrink_pt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_hidden_layers, \n",
    "                 activation, \n",
    "                 lambda_lasso, \n",
    "                 epoch_batch_size, \n",
    "                 batch_size,\n",
    "                 learning_rate, \n",
    "                 num_epochs, \n",
    "                 random_state, \n",
    "                 dropout_prob,\n",
    "                 batch_normalisation = False,\n",
    "                 verbose = False,\n",
    "                 gpu = False,\n",
    "                 gpu_id = 0,\n",
    "                 loss_function='MSE',\n",
    "                 last_activation_function = 'relu',\n",
    "                 **kwargs):\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.activation = activation\n",
    "        self.lambda_lasso = lambda_lasso\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.epoch_batch_size = epoch_batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.verbose = verbose\n",
    "        self.loss_function = loss_function\n",
    "        self.last_activation_function = last_activation_function\n",
    "        self.batch_normalisation = batch_normalisation\n",
    "        \n",
    "        self.gpu = gpu\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "\n",
    "        self.LOSS_FUNCTIONS_MAP = {'MSE': nn.MSELoss(), \n",
    "                                    'MAE': nn.L1Loss(), \n",
    "                                    'Huber': nn.SmoothL1Loss()}\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, train_x, train_y, initial_model=None):\n",
    "\n",
    "        if type(train_y) == pd.core.frame.DataFrame:\n",
    "            self.output_size = len(train_y.columns)\n",
    "        else:\n",
    "            self.output_size = 1\n",
    "        \n",
    "        self.input_size = len(train_x.columns)\n",
    "\n",
    "        gap = (self.input_size - self.output_size)//(self.n_hidden_layers+1)\n",
    "        self.hidden_layer_sizes = [self.input_size - i * gap for i in range(self.n_hidden_layers)]\n",
    "    \n",
    "        # Create the model\n",
    "        self.model = DenseNeuralNetwork(self.n_hidden_layers, \n",
    "                                        self.hidden_layer_sizes, \n",
    "                                        self.activation, \n",
    "                                        self.dropout_prob, \n",
    "                                        self.input_size, \n",
    "                                        self.output_size,\n",
    "                                        self.last_activation_function,\n",
    "                                        self.batch_normalisation)\n",
    "        \n",
    "        if initial_model is not None:\n",
    "            self.model.load_state_dict(initial_model.model.state_dict())\n",
    "        \n",
    "        if self.gpu:\n",
    "            self.model.cuda(self.gpu_id)\n",
    "\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = self.LOSS_FUNCTIONS_MAP['MSE']\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Create the custom datasets\n",
    "        train_dataset = CustomDataset(train_x, train_y)\n",
    "\n",
    "        self.seeds = np.random.randint(10000000, size = self.num_epochs)\n",
    "\n",
    "        \n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.num_epochs):\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, worker_init_fn=lambda _: torch.manual_seed(self.seeds[epoch]))\n",
    "\n",
    "            instances_learnt_so_far = 0\n",
    "\n",
    "            for batch_idx, (batch_train_x, batch_train_y) in enumerate(train_loader):\n",
    "                \n",
    "                if self.gpu:\n",
    "                    batch_train_x, batch_train_y = batch_train_x.cuda(self.gpu_id), batch_train_y.cuda(self.gpu_id)\n",
    "                # Forward pass\n",
    "                outputs = self.model(batch_train_x)\n",
    "                target = batch_train_y.view(-1, 1)  # Reshape target tensor to match the size of the output\n",
    "                loss = criterion(outputs, target)\n",
    "\n",
    "                # Lasso regularization\n",
    "                l1_regularization = torch.tensor(0.)\n",
    "                for param in self.model.parameters():\n",
    "                    l1_regularization += torch.norm(param, p=1)\n",
    "                loss += self.lambda_lasso * l1_regularization\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                instances_learnt_so_far += self.batch_size\n",
    "                \n",
    "                if instances_learnt_so_far >= self.epoch_batch_size:\n",
    "                    break\n",
    "        \n",
    "            # Print the progress\n",
    "            if self.verbose:\n",
    "                if (epoch + 1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        x_tensor = torch.Tensor(x.values)\n",
    "\n",
    "        if self.gpu:\n",
    "            x_tensor = x_tensor.cuda(self.gpu_id)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(x_tensor, training=False)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN_const_pt(n_hidden_layers = 1, \n",
    "                 activation = 'relu', \n",
    "                 lambda_lasso = 0, \n",
    "                 epoch_batch_size = 1000, \n",
    "                 batch_size = 1000,\n",
    "                 learning_rate = 0.1, \n",
    "                 num_epochs = 200, \n",
    "                 random_state = 19260817, \n",
    "                 dropout_prob = 0,\n",
    "                 hidden_layer_n_neuron = 2,\n",
    "                 batch_normalisation = True,\n",
    "                 verbose = True,\n",
    "                 loss_function ='MSE',\n",
    "                 last_activation_function = 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/200], Loss: 1.1194\n",
      "Epoch [200/200], Loss: 1.1100\n"
     ]
    }
   ],
   "source": [
    "model.fit(data[['x1', 'x2']], data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.0253, -1.0685],\n",
       "         [ 0.6198,  0.7410]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.3482,  0.0124], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.4405,  0.3528]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0665], requires_grad=True)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0558707918244834"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "predict = model.predict(data[['x1', 'x2']])\n",
    "\n",
    "predictions = [float(x[0]) for x in predict]\n",
    "r2_score(data['y'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -0.384704\n",
       "1      -0.416314\n",
       "2      -0.056596\n",
       "3       0.915829\n",
       "4       1.055674\n",
       "          ...   \n",
       "9995   -0.540624\n",
       "9996    0.345753\n",
       "9997   -1.387876\n",
       "9998   -2.197660\n",
       "9999   -1.948410\n",
       "Name: y, Length: 10000, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14526790380477905,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14678797125816345,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19516590237617493,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0860733836889267,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2246960997581482,\n",
       " 0.6782543659210205,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1255495250225067,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.33527037501335144,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.010418549180030823,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5756535530090332,\n",
       " 0.15444590151309967,\n",
       " 0.0,\n",
       " 0.16982242465019226,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.003875739872455597,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3394787311553955,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14277370274066925,\n",
       " 0.15242500603199005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16750560700893402,\n",
       " 0.5207821726799011,\n",
       " 0.0,\n",
       " 0.11467058956623077,\n",
       " 0.07092072069644928,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3659733533859253,\n",
       " 0.04576386511325836,\n",
       " 0.0,\n",
       " 0.14024102687835693,\n",
       " 0.27303028106689453,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5224831700325012,\n",
       " 0.040009960532188416,\n",
       " 0.1842065155506134,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6662347912788391,\n",
       " 0.2561276853084564,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14367248117923737,\n",
       " 0.0,\n",
       " 0.15567629039287567,\n",
       " 0.17909245193004608,\n",
       " 0.3106231093406677,\n",
       " 0.0688496083021164,\n",
       " 0.4016075134277344,\n",
       " 0.0,\n",
       " 0.44125282764434814,\n",
       " 0.04521186649799347,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24399936199188232,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3459489941596985,\n",
       " 0.5084654688835144,\n",
       " 0.0,\n",
       " 0.5014516115188599,\n",
       " 0.7200821042060852,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07410654425621033,\n",
       " 0.04174317419528961,\n",
       " 0.0,\n",
       " 0.09628798067569733,\n",
       " 0.042716674506664276,\n",
       " 0.4204831123352051,\n",
       " 0.742481529712677,\n",
       " 0.0,\n",
       " 0.5338631272315979,\n",
       " 0.06060628592967987,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.28451013565063477,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08988474309444427,\n",
       " 0.4007294476032257,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.357849657535553,\n",
       " 0.0,\n",
       " 0.156717449426651,\n",
       " 0.0,\n",
       " 0.24719196557998657,\n",
       " 0.0,\n",
       " 0.15304173529148102,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07475543022155762,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2814037799835205,\n",
       " 0.0,\n",
       " 0.5686279535293579,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2459574043750763,\n",
       " 0.28787127137184143,\n",
       " 0.0,\n",
       " 0.029658757150173187,\n",
       " 0.0,\n",
       " 0.1642320305109024,\n",
       " 0.07089366018772125,\n",
       " 0.07914678752422333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2229636013507843,\n",
       " 0.0,\n",
       " 0.6738946437835693,\n",
       " 0.2512710392475128,\n",
       " 0.0,\n",
       " 0.014832265675067902,\n",
       " 0.19220083951950073,\n",
       " 0.04712437838315964,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7910564541816711,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09477624297142029,\n",
       " 0.27018311619758606,\n",
       " 0.21125084161758423,\n",
       " 0.13453438878059387,\n",
       " 0.0,\n",
       " 0.10481756925582886,\n",
       " 0.0,\n",
       " 0.15548820793628693,\n",
       " 0.3400290012359619,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2725178301334381,\n",
       " 0.13538266718387604,\n",
       " 0.6545507907867432,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.44947850704193115,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.27248334884643555,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05305875837802887,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14984886348247528,\n",
       " 0.38048166036605835,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24662619829177856,\n",
       " 0.15510381758213043,\n",
       " 0.0,\n",
       " 0.1332443654537201,\n",
       " 0.0,\n",
       " 0.15179838240146637,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3683145046234131,\n",
       " 0.3794083893299103,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13906627893447876,\n",
       " 0.0,\n",
       " 0.22291281819343567,\n",
       " 0.07566219568252563,\n",
       " 0.6644112467765808,\n",
       " 0.12862013280391693,\n",
       " 0.0,\n",
       " 0.6963271498680115,\n",
       " 0.3604496419429779,\n",
       " 0.0266360342502594,\n",
       " 0.0641661286354065,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.013624846935272217,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10310667753219604,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1286022961139679,\n",
       " 0.0,\n",
       " 0.28317779302597046,\n",
       " 0.0,\n",
       " 0.47982311248779297,\n",
       " 0.32409900426864624,\n",
       " 0.07140286266803741,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2138083577156067,\n",
       " 0.14780665934085846,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.29832279682159424,\n",
       " 0.0,\n",
       " 0.018124215304851532,\n",
       " 0.0,\n",
       " 0.04183107614517212,\n",
       " 0.19197481870651245,\n",
       " 0.046243637800216675,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.011502556502819061,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1543787717819214,\n",
       " 0.0,\n",
       " 0.11733092367649078,\n",
       " 0.0,\n",
       " 0.20965605974197388,\n",
       " 0.3319471776485443,\n",
       " 0.0,\n",
       " 0.0771084874868393,\n",
       " 0.0,\n",
       " 0.06365792453289032,\n",
       " 0.08483891189098358,\n",
       " 0.0,\n",
       " 0.3762698769569397,\n",
       " 0.4842694401741028,\n",
       " 0.05609232932329178,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10039165616035461,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.034856125712394714,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3074081540107727,\n",
       " 0.03549860417842865,\n",
       " 0.24737149477005005,\n",
       " 0.0,\n",
       " 0.5250521302223206,\n",
       " 0.5413169264793396,\n",
       " 0.3113574683666229,\n",
       " 0.0,\n",
       " 0.8140352368354797,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07744385302066803,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.055218927562236786,\n",
       " 0.06795786321163177,\n",
       " 0.0,\n",
       " 0.020365215837955475,\n",
       " 0.0,\n",
       " 0.0994788110256195,\n",
       " 0.0,\n",
       " 0.6993780136108398,\n",
       " 0.030710190534591675,\n",
       " 0.34788236021995544,\n",
       " 0.0,\n",
       " 0.5659329295158386,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.22214993834495544,\n",
       " 0.21461725234985352,\n",
       " 0.24222984910011292,\n",
       " 0.0,\n",
       " 0.22628527879714966,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6479509472846985,\n",
       " 0.41939428448677063,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6289005875587463,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24436044692993164,\n",
       " 0.17930038273334503,\n",
       " 0.06220850348472595,\n",
       " 0.055084578692913055,\n",
       " 0.0,\n",
       " 0.41141870617866516,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1114172637462616,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2746354639530182,\n",
       " 0.632043182849884,\n",
       " 0.12581495940685272,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5963547825813293,\n",
       " 0.0,\n",
       " 0.2746318280696869,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6665186882019043,\n",
       " 0.6442126631736755,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1211504191160202,\n",
       " 0.0,\n",
       " 0.5382637977600098,\n",
       " 0.0,\n",
       " 0.09369532763957977,\n",
       " 0.5359398722648621,\n",
       " 0.20990517735481262,\n",
       " 0.0071587935090065,\n",
       " 0.0,\n",
       " 0.06573683023452759,\n",
       " 0.23900502920150757,\n",
       " 0.1461726427078247,\n",
       " 0.0,\n",
       " 0.6115073561668396,\n",
       " 0.26504313945770264,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07732558250427246,\n",
       " 0.38221651315689087,\n",
       " 0.28198155760765076,\n",
       " 0.5241790413856506,\n",
       " 0.015719331800937653,\n",
       " 0.37828364968299866,\n",
       " 0.26306983828544617,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13942445814609528,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1359100341796875,\n",
       " 0.18331825733184814,\n",
       " 0.2507869303226471,\n",
       " 0.25074079632759094,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.28398755192756653,\n",
       " 0.3601171374320984,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5335800647735596,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1703721582889557,\n",
       " 0.060547009110450745,\n",
       " 0.3696489632129669,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2702336311340332,\n",
       " 0.0,\n",
       " 0.11846697330474854,\n",
       " 0.42283934354782104,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04732175171375275,\n",
       " 0.39826375246047974,\n",
       " 0.06734205782413483,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14646963775157928,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08457225561141968,\n",
       " 0.024129778146743774,\n",
       " 0.024131223559379578,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04894815385341644,\n",
       " 0.0,\n",
       " 0.14233356714248657,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.20514625310897827,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.23380056023597717,\n",
       " 0.3650391399860382,\n",
       " 0.0,\n",
       " 0.2744080424308777,\n",
       " 0.009057030081748962,\n",
       " 0.3104909062385559,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08518053591251373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16707320511341095,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.026947960257530212,\n",
       " 0.0,\n",
       " 0.10132402181625366,\n",
       " 0.21032658219337463,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.31974560022354126,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3938274085521698,\n",
       " 0.0,\n",
       " 0.030776582658290863,\n",
       " 0.0399957001209259,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2088804543018341,\n",
       " 0.3834979236125946,\n",
       " 0.062231793999671936,\n",
       " 0.720154345035553,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3099796772003174,\n",
       " 0.0,\n",
       " 0.10936261713504791,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.18548712134361267,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09441722929477692,\n",
       " 0.4823077321052551,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09043800830841064,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02008911222219467,\n",
       " 0.3269454836845398,\n",
       " 0.05214438587427139,\n",
       " 0.2375483512878418,\n",
       " 0.36844485998153687,\n",
       " 0.0,\n",
       " 0.22044092416763306,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.18710440397262573,\n",
       " 0.3092571496963501,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.12522338330745697,\n",
       " 0.4654760956764221,\n",
       " 0.5309606790542603,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.348436176776886,\n",
       " 0.18341779708862305,\n",
       " 0.0,\n",
       " 0.16168363392353058,\n",
       " 0.046677276492118835,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1599336564540863,\n",
       " 0.0,\n",
       " 0.5191783308982849,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3571884334087372,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3107147514820099,\n",
       " 0.0,\n",
       " 0.47406500577926636,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30947086215019226,\n",
       " 0.12148334085941315,\n",
       " 0.7357558012008667,\n",
       " 0.20553338527679443,\n",
       " 0.19344410300254822,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.44369620084762573,\n",
       " 0.07997345924377441,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005415752530097961,\n",
       " 0.13735371828079224,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.47423338890075684,\n",
       " 0.12265323102474213,\n",
       " 0.0,\n",
       " 0.1051730215549469,\n",
       " 0.18200860917568207,\n",
       " 0.23508906364440918,\n",
       " 0.21241417527198792,\n",
       " 0.40984484553337097,\n",
       " 0.0,\n",
       " 0.14390970766544342,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3616836369037628,\n",
       " 0.0,\n",
       " 0.5568843483924866,\n",
       " 0.0,\n",
       " 0.42490148544311523,\n",
       " 0.2389959692955017,\n",
       " 0.0,\n",
       " 0.2753281593322754,\n",
       " 0.40640220046043396,\n",
       " 0.36287498474121094,\n",
       " 0.1086883395910263,\n",
       " 0.10923922061920166,\n",
       " 0.006818518042564392,\n",
       " 0.19865760207176208,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09127466380596161,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3162771165370941,\n",
       " 0.012681588530540466,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11136472225189209,\n",
       " 0.371703177690506,\n",
       " 0.23091813921928406,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3411712050437927,\n",
       " 0.0,\n",
       " 0.06333865225315094,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.35818564891815186,\n",
       " 0.0,\n",
       " 0.4922422170639038,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07135927677154541,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.008433952927589417,\n",
       " 0.0,\n",
       " 0.040736123919487,\n",
       " 0.3400264084339142,\n",
       " 0.15390154719352722,\n",
       " 0.0,\n",
       " 0.30687955021858215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14133453369140625,\n",
       " 0.5949878692626953,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.34413883090019226,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.26644930243492126,\n",
       " 0.0,\n",
       " 0.317086786031723,\n",
       " 0.0,\n",
       " 0.0611506849527359,\n",
       " 0.5089977383613586,\n",
       " 0.155098557472229,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38668009638786316,\n",
       " 0.3406960070133209,\n",
       " 0.6360323429107666,\n",
       " 0.44113248586654663,\n",
       " 0.0,\n",
       " 0.25266164541244507,\n",
       " 0.0,\n",
       " 0.5126137137413025,\n",
       " 0.0,\n",
       " 0.1668025106191635,\n",
       " 0.3209677040576935,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.23426362872123718,\n",
       " 0.020600304007530212,\n",
       " 0.0,\n",
       " 0.18468385934829712,\n",
       " 0.10120309889316559,\n",
       " 0.3509685695171356,\n",
       " 0.0884333997964859,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2426726222038269,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6196073293685913,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05620771646499634,\n",
       " 0.2688734531402588,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.48792892694473267,\n",
       " 0.1427135467529297,\n",
       " 0.9161249399185181,\n",
       " 0.25714099407196045,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0563771054148674,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1279173046350479,\n",
       " 1.1271114349365234,\n",
       " 0.0,\n",
       " 0.018177658319473267,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.35194942355155945,\n",
       " 0.9180638790130615,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10154254734516144,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07706323266029358,\n",
       " 0.0,\n",
       " 0.47647666931152344,\n",
       " 0.0,\n",
       " 0.17459729313850403,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08633129298686981,\n",
       " 0.5700439214706421,\n",
       " 0.3377474844455719,\n",
       " 0.1889648735523224,\n",
       " 0.10286299884319305,\n",
       " 0.0,\n",
       " 0.5041002035140991,\n",
       " 0.0,\n",
       " 0.0818728655576706,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5623579621315002,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5577268600463867,\n",
       " 0.0,\n",
       " 0.23553013801574707,\n",
       " 0.07520033419132233,\n",
       " 0.35252633690834045,\n",
       " 0.01118408888578415,\n",
       " 0.2207396924495697,\n",
       " 0.21999257802963257,\n",
       " 0.19248339533805847,\n",
       " 0.0,\n",
       " 0.3820061981678009,\n",
       " 0.27284741401672363,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16176949441432953,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2761421203613281,\n",
       " 0.035608358681201935,\n",
       " 0.0,\n",
       " 0.185805082321167,\n",
       " 0.0,\n",
       " 0.7045320868492126,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.261588990688324,\n",
       " 0.17086629569530487,\n",
       " 0.24884110689163208,\n",
       " 0.31261345744132996,\n",
       " 0.0,\n",
       " 0.13259166479110718,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6349886655807495,\n",
       " 0.19431394338607788,\n",
       " 0.22816279530525208,\n",
       " 0.0,\n",
       " 0.1235792487859726,\n",
       " 0.133306622505188,\n",
       " 0.12972430884838104,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.20255863666534424,\n",
       " 0.0,\n",
       " 0.620601236820221,\n",
       " 0.21665504574775696,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.27225160598754883,\n",
       " 0.0,\n",
       " 0.10727836191654205,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01618756353855133,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.39833658933639526,\n",
       " 0.0,\n",
       " 0.18993628025054932,\n",
       " 0.0,\n",
       " 0.49922311305999756,\n",
       " 0.07661181688308716,\n",
       " 0.17909996211528778,\n",
       " 0.11868557333946228,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25378096103668213,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6309523582458496,\n",
       " 0.3969774842262268,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05397459864616394,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11424088478088379,\n",
       " 0.04669445753097534,\n",
       " 0.0,\n",
       " 0.16697266697883606,\n",
       " 0.0,\n",
       " 0.24671012163162231,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04040670394897461,\n",
       " 0.0,\n",
       " 0.05188269168138504,\n",
       " 0.0,\n",
       " 0.10699926316738129,\n",
       " 0.47242802381515503,\n",
       " 0.0,\n",
       " 0.3312961161136627,\n",
       " 0.24751731753349304,\n",
       " 0.03278055787086487,\n",
       " 0.2195892333984375,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03196554630994797,\n",
       " 0.39502981305122375,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6283720135688782,\n",
       " 0.0,\n",
       " 0.14164872467517853,\n",
       " 0.07276718318462372,\n",
       " 0.4719163775444031,\n",
       " 0.1635655015707016,\n",
       " 0.5677279829978943,\n",
       " 0.2470148205757141,\n",
       " 0.29362183809280396,\n",
       " 0.5579289793968201,\n",
       " 0.5925136804580688,\n",
       " 0.12181873619556427,\n",
       " 0.42913976311683655,\n",
       " 0.0,\n",
       " 0.7692831158638,\n",
       " 0.0,\n",
       " 0.2694931626319885,\n",
       " 0.21608778834342957,\n",
       " 0.013078257441520691,\n",
       " 0.2133101522922516,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.22400078177452087,\n",
       " 0.0,\n",
       " 0.15677757561206818,\n",
       " 0.01471707969903946,\n",
       " 0.02412070333957672,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24919870495796204,\n",
       " 0.0,\n",
       " 0.4837690591812134,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05391845852136612,\n",
       " 0.23940271139144897,\n",
       " 0.32294762134552,\n",
       " 0.4898548722267151,\n",
       " 0.021431632339954376,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.44441741704940796,\n",
       " 0.0,\n",
       " 0.0039001181721687317,\n",
       " 0.19766384363174438,\n",
       " 0.02968420833349228,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.045275308191776276,\n",
       " 0.6625937819480896,\n",
       " 0.0,\n",
       " 0.16930854320526123,\n",
       " 0.14283113181591034,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2559908926486969,\n",
       " 0.0,\n",
       " 0.43544358015060425,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0821399986743927,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13608703017234802,\n",
       " 0.0,\n",
       " 0.08558347821235657,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.030705899000167847,\n",
       " 0.0,\n",
       " 0.14909449219703674,\n",
       " 0.0,\n",
       " 0.9615032076835632,\n",
       " 0.15875573456287384,\n",
       " 0.26061171293258667,\n",
       " 0.1948649287223816,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.007745951414108276,\n",
       " 0.34040093421936035,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.33958595991134644,\n",
       " 0.0,\n",
       " 0.11005845665931702,\n",
       " 0.5535534024238586,\n",
       " ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression().fit(data[['x1', 'x2']], data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11290521370902162"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(data['y'], model.predict(data[['x1', 'x2']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
